{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss, auc\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from math import *\n",
    "import numbers\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "# ML model algorithms\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "# import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score,recall_score, roc_auc_score, roc_curve, log_loss, r2_score, confusion_matrix, classification_report, auc, cohen_kappa_score, f1_score, matthews_corrcoef #evaluation metrics \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# for save and load model\n",
    "import pickle\n",
    "\n",
    "# from sklearn import svm\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plotly_bar_chart_figwidth(n):\n",
    "    \"\"\"Heuristic to determine suitable plotly figwidth base on number of category.\"\"\"\n",
    "    width = 300 if (n <= 2) else n*max(50, 140*np.power(0.92, n/2))\n",
    "    return width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits\n",
    "from sklearn.utils.validation import check_random_state, column_or_1d\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from collections import defaultdict\n",
    "\n",
    "class StratifiedGroupKFold(_BaseKFold):\n",
    "    \"\"\"Stratified K-Folds iterator variant with non-overlapping groups.\n",
    "    This cross-validation object is a variation of StratifiedKFold attempts to\n",
    "    return stratified folds with non-overlapping groups. The folds are made by\n",
    "    preserving the percentage of samples for each class.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    The difference between GroupKFold and StratifiedGroupKFold is that\n",
    "    the former attempts to create balanced folds such that the number of\n",
    "    distinct groups is approximately the same in each fold, whereas\n",
    "    StratifiedGroupKFold attempts to create folds which preserve the\n",
    "    percentage of samples for each class as much as possible given the\n",
    "    constraint of non-overlapping groups between splits.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of folds. Must be at least 2.\n",
    "    shuffle : bool, default=False\n",
    "        Whether to shuffle each class's samples before splitting into batches.\n",
    "        Note that the samples within each split will not be shuffled.\n",
    "        This implementation can only shuffle groups that have approximately the\n",
    "        same y distribution, no global shuffle will be performed.\n",
    "    random_state : int or RandomState instance, default=None\n",
    "        When `shuffle` is True, `random_state` affects the ordering of the\n",
    "        indices, which controls the randomness of each fold for each class.\n",
    "        Otherwise, leave `random_state` as `None`.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import StratifiedGroupKFold\n",
    "    >>> X = np.ones((17, 2))\n",
    "    >>> y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    >>> groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])\n",
    "    >>> cv = StratifiedGroupKFold(n_splits=3)\n",
    "    >>> for train_idxs, test_idxs in cv.split(X, y, groups):\n",
    "    ...     print(\"TRAIN:\", groups[train_idxs])\n",
    "    ...     print(\"      \", y[train_idxs])\n",
    "    ...     print(\" TEST:\", groups[test_idxs])\n",
    "    ...     print(\"      \", y[test_idxs])\n",
    "    TRAIN: [1 1 2 2 4 5 5 5 5 8 8]\n",
    "           [0 0 1 1 1 0 0 0 0 0 0]\n",
    "     TEST: [3 3 3 6 6 7]\n",
    "           [1 1 1 0 0 0]\n",
    "    TRAIN: [3 3 3 4 5 5 5 5 6 6 7]\n",
    "           [1 1 1 1 0 0 0 0 0 0 0]\n",
    "     TEST: [1 1 2 2 8 8]\n",
    "           [0 0 1 1 0 0]\n",
    "    TRAIN: [1 1 2 2 3 3 3 6 6 7 8 8]\n",
    "           [0 0 1 1 1 1 1 0 0 0 0 0]\n",
    "     TEST: [4 5 5 5 5]\n",
    "           [1 0 0 0 0]\n",
    "    Notes\n",
    "    -----\n",
    "    The implementation is designed to:\n",
    "    * Mimic the behavior of StratifiedKFold as much as possible for trivial\n",
    "      groups (e.g. when each group contains only one sample).\n",
    "    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n",
    "      ``y = [1, 0]`` should not change the indices generated.\n",
    "    * Stratify based on samples as much as possible while keeping\n",
    "      non-overlapping groups constraint. That means that in some cases when\n",
    "      there is a small number of groups containing a large number of samples\n",
    "      the stratification will not be possible and the behavior will be close\n",
    "      to GroupKFold.\n",
    "    See also\n",
    "    --------\n",
    "    StratifiedKFold: Takes class information into account to build folds which\n",
    "        retain class distributions (for binary or multiclass classification\n",
    "        tasks).\n",
    "    GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle,\n",
    "                         random_state=random_state)\n",
    "\n",
    "    def _iter_test_indices(self, X, y, groups):\n",
    "        # Implementation is based on this kaggle kernel:\n",
    "        # https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n",
    "        # and is a subject to Apache 2.0 License. You may obtain a copy of the\n",
    "        # License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "        # Changelist:\n",
    "        # - Refactored function to a class following scikit-learn KFold\n",
    "        #   interface.\n",
    "        # - Added heuristic for assigning group to the least populated fold in\n",
    "        #   cases when all other criteria are equal\n",
    "        # - Swtch from using python ``Counter`` to ``np.unique`` to get class\n",
    "        #   distribution\n",
    "        # - Added scikit-learn checks for input: checking that target is binary\n",
    "        #   or multiclass, checking passed random state, checking that number\n",
    "        #   of splits is less than number of members in each class, checking\n",
    "        #   that least populated class has more members than there are splits.\n",
    "        rng = check_random_state(self.random_state)\n",
    "        y = np.asarray(y)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "        allowed_target_types = ('binary', 'multiclass')\n",
    "        if type_of_target_y not in allowed_target_types:\n",
    "            raise ValueError(\n",
    "                'Supported target types are: {}. Got {!r} instead.'.format(\n",
    "                    allowed_target_types, type_of_target_y))\n",
    "\n",
    "        y = column_or_1d(y)\n",
    "        _, y_inv, y_cnt = np.unique(y, return_inverse=True, return_counts=True)\n",
    "        if np.all(self.n_splits > y_cnt):\n",
    "            raise ValueError(\"n_splits=%d cannot be greater than the\"\n",
    "                             \" number of members in each class.\"\n",
    "                             % (self.n_splits))\n",
    "        n_smallest_class = np.min(y_cnt)\n",
    "        if self.n_splits > n_smallest_class:\n",
    "            warnings.warn((\"The least populated class in y has only %d\"\n",
    "                           \" members, which is less than n_splits=%d.\"\n",
    "                           % (n_smallest_class, self.n_splits)), UserWarning)\n",
    "        n_classes = len(y_cnt)\n",
    "        \n",
    "        \n",
    "        _, groups_inv, groups_cnt = np.unique(\n",
    "            groups, return_inverse=True, return_counts=True)\n",
    "        y_counts_per_group = np.zeros((len(groups_cnt), n_classes))\n",
    "        for class_idx, group_idx in zip(y_inv, groups_inv):\n",
    "            y_counts_per_group[group_idx, class_idx] += 1\n",
    "\n",
    "        y_counts_per_fold = np.zeros((self.n_splits, n_classes))\n",
    "        groups_per_fold = defaultdict(set)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(y_counts_per_group)\n",
    "\n",
    "        # Stable sort to keep shuffled order for groups with the same\n",
    "        # class distribution variance\n",
    "        sorted_groups_idx = np.argsort(-np.std(y_counts_per_group, axis=1),\n",
    "                                       kind='mergesort')\n",
    "\n",
    "        for group_idx in sorted_groups_idx:\n",
    "            group_y_counts = y_counts_per_group[group_idx]\n",
    "            best_fold = self._find_best_fold(\n",
    "                y_counts_per_fold=y_counts_per_fold, y_cnt=y_cnt,\n",
    "                group_y_counts=group_y_counts)\n",
    "            y_counts_per_fold[best_fold] += group_y_counts\n",
    "            groups_per_fold[best_fold].add(group_idx)\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            test_indices = [idx for idx, group_idx in enumerate(groups_inv)\n",
    "                            if group_idx in groups_per_fold[i]]\n",
    "            yield test_indices\n",
    "\n",
    "    def _find_best_fold(\n",
    "            self, y_counts_per_fold, y_cnt, group_y_counts):\n",
    "        best_fold = None\n",
    "        min_eval = np.inf\n",
    "        min_samples_in_fold = np.inf\n",
    "        for i in range(self.n_splits):\n",
    "            y_counts_per_fold[i] += group_y_counts\n",
    "            # Summarise the distribution over classes in each proposed fold\n",
    "            std_per_class = np.std(\n",
    "                y_counts_per_fold / y_cnt.reshape(1, -1),\n",
    "                axis=0)\n",
    "            y_counts_per_fold[i] -= group_y_counts\n",
    "            fold_eval = np.mean(std_per_class)\n",
    "            samples_in_fold = np.sum(y_counts_per_fold[i])\n",
    "            is_current_fold_better = (\n",
    "                fold_eval < min_eval or\n",
    "                np.isclose(fold_eval, min_eval)\n",
    "                and samples_in_fold < min_samples_in_fold\n",
    "            )\n",
    "            if is_current_fold_better:\n",
    "                min_eval = fold_eval\n",
    "                min_samples_in_fold = samples_in_fold\n",
    "                best_fold = i\n",
    "        return best_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_feature_importance_group(classifiers, cols, test_index, max_display=10):\n",
    "    list_explainer = [shap.TreeExplainer(m) for m in classifiers]\n",
    "    list_shap_values = [explainer.shap_values(test[cols]) for explainer, test in zip(list_explainer, test_index)]\n",
    "\n",
    "    list_vals = [np.abs(shap_values[1]).mean(0) for shap_values in list_shap_values]\n",
    "\n",
    "    num_models = len(list_vals)\n",
    "    list_feature_importance_col = [f\"feature_importance_vals_model_{i + 1}\" for i in range(num_models)]\n",
    "    \n",
    "    # Display the SHAP summary plot\n",
    "    shap.summary_plot(list_shap_values[-1][1], test_index[-1][cols], max_display=max_display, plot_type=\"dot\", plot_size=(20,20))\n",
    "\n",
    "    # Create DataFrame with feature importances\n",
    "    feature_importance_df = pd.DataFrame(list(zip(cols, *list_vals)), columns=[\"feature_name\"] + list_feature_importance_col)\n",
    "    \n",
    "    # Set feature_name as index\n",
    "    feature_importance_df.set_index(\"feature_name\", inplace=True)\n",
    "    \n",
    "    # Normalize the importances to sum up to 100 for each model\n",
    "    feature_importance_df = feature_importance_df.div(feature_importance_df.sum(axis=0), axis=1) * 100\n",
    "    \n",
    "    # Calculate mean and std of importances\n",
    "    feature_importance_df[\"mean_importance\"] = feature_importance_df.mean(axis=1)\n",
    "    feature_importance_df[\"std_importance\"] = feature_importance_df.std(axis=1)\n",
    "    \n",
    "    # Sort values by mean_importance\n",
    "    feature_importance_df.sort_values(\"mean_importance\", ascending=False, inplace=True)\n",
    "    \n",
    "    # Reset index to get feature_name as a column again\n",
    "    feature_importance_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Select relevant columns\n",
    "    result_df = feature_importance_df[[\"feature_name\", \"mean_importance\", \"std_importance\"] + list_feature_importance_col]\n",
    "    \n",
    "    return list_shap_values, result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_feature_importance_regression(classifiers, cols, test_index, max_display=10):\n",
    "    list_explainer = [shap.TreeExplainer(m) for m in classifiers]\n",
    "    \n",
    "    # Calculate SHAP values for each fold\n",
    "    list_shap_values = [explainer.shap_values(test[cols]) for explainer, test in zip(list_explainer, test_index)]\n",
    "    \n",
    "    # Extracting the SHAP values for the predicted output (assuming single output)\n",
    "    list_vals = [np.abs(shap_values).mean(0) for shap_values in list_shap_values]\n",
    "    \n",
    "    num_models = len(list_vals)\n",
    "    list_feature_importance_col = [f\"feature_importance_vals_model_{i + 1}\" for i in range(num_models)]\n",
    "    \n",
    "    # Display the SHAP summary plot for the last fold\n",
    "    shap.summary_plot(list_shap_values[-1], test_index[-1][cols], max_display=max_display, plot_type=\"dot\", plot_size=(20,20))\n",
    "\n",
    "    # Create DataFrame with feature importances\n",
    "    feature_importance_df = pd.DataFrame(list(zip(cols, *list_vals)), columns=[\"feature_name\"] + list_feature_importance_col)\n",
    "    \n",
    "    # Set feature_name as index\n",
    "    feature_importance_df.set_index(\"feature_name\", inplace=True)\n",
    "    \n",
    "    # Normalize the importances to sum up to 100 for each model\n",
    "    feature_importance_df = feature_importance_df.div(feature_importance_df.sum(axis=0), axis=1) * 100\n",
    "    \n",
    "    # Calculate mean and std of importances\n",
    "    feature_importance_df[\"mean_importance\"] = feature_importance_df.mean(axis=1)\n",
    "    feature_importance_df[\"std_importance\"] = feature_importance_df.std(axis=1)\n",
    "    \n",
    "    # Sort values by mean_importance\n",
    "    feature_importance_df.sort_values(\"mean_importance\", ascending=False, inplace=True)\n",
    "    \n",
    "    # Reset index to get feature_name as a column again\n",
    "    feature_importance_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Select relevant columns\n",
    "    result_df = feature_importance_df[[\"feature_name\", \"mean_importance\", \"std_importance\"] + list_feature_importance_col]\n",
    "    \n",
    "    return list_shap_values, result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_compare(df,col=['label']):\n",
    "    a = df[col].value_counts().rename(\"count\").reset_index()\n",
    "\n",
    "    b = df[col].value_counts(normalize=True).rename(\"proportion\").reset_index()\n",
    "    return pd.merge(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_compare_group(df, cols, group_col='subset'):\n",
    "    a = df[cols].groupby(group_col, as_index=False).value_counts()\n",
    "\n",
    "    b = df[cols].groupby(group_col, as_index=False).value_counts(normalize=True)\n",
    "\n",
    "    return pd.merge(a, b).sort_values(by=cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_style.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_corr_pairs(corrMatrix, threshold):\n",
    "    corr_filt = corrMatrix[corrMatrix > threshold].replace({1.0:np.nan}).dropna(how=\"all\").dropna(how=\"all\", axis = 1)\n",
    "    columns_pairs = []\n",
    "\n",
    "    for index, row in corr_filt.iterrows():\n",
    "        for columnIndex, value in row.items():\n",
    "            if (str(value) != 'nan') and ([columnIndex, index, value] not in columns_pairs):\n",
    "                columns_pairs.append([index,columnIndex, value])\n",
    "\n",
    "    corr_df = pd.DataFrame(columns_pairs).drop_duplicates(subset=[0,1]).sort_values(by = [2], ascending = False)\n",
    "    display(corr_df.rename(columns={0:'feature 1', 1:'feature 2', 2: 'corr'}).reset_index(drop=True).style.background_gradient(subset=['corr']))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "def drop_high_corr_cols(df_corr, threshold=0.9):\n",
    "    corr_matrix = df_corr.abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "   # Find index of feature columns with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    print(\"Number of drop columns: \", len(to_drop))\n",
    "    \n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_single_var(train, test, cols, label=\"label\", params={}):\n",
    "    \n",
    "    y_train = train[label]\n",
    "    y_test = test[label]\n",
    "    \n",
    "    aucs = []\n",
    "    for col in cols:\n",
    "        X_transform = train[[col]]\n",
    "        classifier = lgb.LGBMClassifier(**params)\n",
    "        classifier.fit(X_transform, y_train)\n",
    "        \n",
    "        X_test = test[[col]]\n",
    "        lr_probs = classifier.predict_proba(X_test)[:, 1]\n",
    "        lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "        aucs.append(lr_auc)\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(cols, aucs)), columns =['feature_name', 'auc'])\n",
    "    \n",
    "    df.sort_values(by=\"auc\", ascending=False, inplace=True)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def write_log(title, model_params, src_log, feature_importance,\n",
    "              val_results, train_results, pred_val, pred_train, add_feature=[], drop_cols=[]):\n",
    "    \n",
    "    \n",
    "    #round scores to 4 decimals\n",
    "    val_results = [round(num, 4) for num in val_results]\n",
    "    train_results = [round(num, 4) for num in train_results]\n",
    "    \n",
    "    #create a write dict\n",
    "    write_dict = {  'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                    'title': title,\n",
    "                    'logs': ', '.join(src_log),\n",
    "                    'model_params': json.dumps(model_params),\n",
    "                    'number_of_feature': feature_importance.shape[0],\n",
    "                    'number_of_features_added': len(add_feature),\n",
    "                    'features_added': add_feature,\n",
    "                    'features': feature_importance['feature_name'].to_list(),\n",
    "                    'feature_importance': feature_importance['mean_importance'].to_list(),\n",
    "                    'train_auc': train_results,\n",
    "                    'train_auc_mean': round(np.mean(train_results), 4),\n",
    "                    'train_auc_std': round(np.std(train_results), 4),\n",
    "                    'val_auc': val_results,\n",
    "                    'val_auc_mean': round(np.mean(val_results), 4),\n",
    "                    'val_auc_std': round(np.std(val_results), 4),\n",
    "                    'drop_columns': len(drop_cols),\n",
    "                    'train_auc_change': round(np.mean(train_results) - np.mean(pred_train), 4),\n",
    "                    'val_auc_change': round(np.mean(val_results) - np.mean(pred_val), 4)\n",
    "                 }\n",
    "    \n",
    "    \n",
    "    #write log\n",
    "    path = 'log/model_log'\n",
    "    with open(path, 'a') as f: \n",
    "        for key, value in write_dict.items(): \n",
    "            f.write('%18s: %s\\n' % (key, value))\n",
    "        f.write('-----------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_1d_num(df, col=\"x\", draw=False):\n",
    "    data = df[col]\n",
    "    percentiles = [i * 0.1 for i in range(1, 10)] + [0.01, 0.05, 0.95, 0.99]\n",
    "    report = data.describe(percentiles= percentiles).rename(0).to_frame().T\n",
    "#     report.drop(columns=[\"0%\"], inplace=True) \n",
    "    \n",
    "    report.insert(3,'kurt',round(data.kurt(),2))\n",
    "    report.insert(3,'skew',round(data.skew(),2))\n",
    "    report.insert(1,'mode',data.mode()[0])\n",
    "    report.insert(1,'nan',data.isnull().sum() / data.shape[0])\n",
    "\n",
    "    if draw:\n",
    "        fig1 = ff.create_distplot([data], [\"Distribution of \"+col], show_rug=False)\n",
    "    #     fig1 = px.histogram(x=data, nbins=10)\n",
    "    #     fig = plt.figure(figsize=(10,5))\n",
    "    #     sns.displot(x=data, kind=\"kde\")\n",
    "    #     plt.close()\n",
    "    #     fig2 = px.histogram(x=data, nbins=100)\n",
    "\n",
    "        return thanhnm3_hstack_sbs([report.T.rename(columns={0:col}), fig1])\n",
    "    return report.T.rename(columns={0:col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_cont(df, cols):\n",
    "    corr = df[cols].corr()\n",
    "    \n",
    "    cmap=sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True)\n",
    "\n",
    "\n",
    "    def magnify():\n",
    "        return [dict(selector=\"th\",\n",
    "                     props=[(\"font-size\", \"7pt\")]),\n",
    "                dict(selector=\"td\",\n",
    "                     props=[('padding', \"0em 0em\")]),\n",
    "                dict(selector=\"th:hover\",\n",
    "                     props=[(\"font-size\", \"12pt\")]),\n",
    "                dict(selector=\"tr:hover td:hover\",\n",
    "                     props=[('max-width', '500px'),\n",
    "                            ('font-size', '12pt')])\n",
    "    ]\n",
    "\n",
    "    corr_style = corr.style.background_gradient(cmap, axis=1)\\\n",
    "        .set_properties(**{'max-width': '100px', 'font-size': '10pt'})\\\n",
    "        .set_caption(\"Correlation between continous columns\")\\\n",
    "        .set_precision(2)\n",
    "    \n",
    "    return corr_style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_2d_num_num(df, col1, col2):\n",
    "    corr_style = cont_cont(df, [col1, col2])\n",
    "    display(corr_style)\n",
    "    \n",
    "    df1 = describe_1d_num(df, col1, draw=False)\n",
    "    df2 = describe_1d_num(df, col2, draw=False)\n",
    "    df_concat = pd.concat([df1, df2], axis=1)\n",
    "    \n",
    "    fig = ff.create_distplot([df[col1], df[col2]], [col1, col2], bin_size=.5, show_hist=False, show_rug=False,colors=[\"gray\", \"blue\"] )\n",
    "\n",
    "    fig.update_layout(title_text=f\"Distribution of {col1} and {col2}\")\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "#     fig.update_layout({\n",
    "#     \"plot_bgcolor\": \"rgba(0, 0, 0, 0)\",\n",
    "#     \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",\n",
    "#     })\n",
    "    fig.update_layout(\n",
    "        title=f\"Distribution of {col1} and {col2}\",\n",
    "        xaxis_title=\"pred_prob\",\n",
    "        yaxis_title=\"density\",\n",
    "    #     legend_title=\"Legend Title\",\n",
    "    #     font=dict(\n",
    "    #         family=\"Courier New, monospace\",\n",
    "    #         size=18,\n",
    "    #         color=\"RebeccaPurple\"\n",
    "    #     )\n",
    "    )\n",
    "    \n",
    "    return thanhnm3_hstack_sbs([df_concat, fig])\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_production(df_pd):\n",
    "    \n",
    "    df_eval = pd.DataFrame({\"prob\": df_pd['pred_prob'], \"true\":df_pd['label'].astype(bool)})\n",
    "    df_eval = df_eval.sort_values(by=['prob'], ascending = False)\n",
    "    \n",
    "    y_pos_splits = np.array_split(np.array(df_eval['prob']), 10)\n",
    "    y_true_splits = np.array_split(np.array(df_eval['true']), 10)\n",
    "    \n",
    "    print(\"Head of predictions:\")\n",
    "    print(display(df_eval.reset_index(drop=True).head(20)))\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    sb.displot(df_eval[df_eval['true']==1]['prob'])\n",
    "    sb.displot(df_eval[df_eval['true']==0]['prob'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cf_matrix(y_test, y_pred, threshold):\n",
    "    \n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "    ax.set_title('Confusion Matrix with labels at threshold {0:0.2f}\\n\\n'.format(threshold));\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ');\n",
    "\n",
    "    ## Ticket labels - List must be in alphabetical order\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    ## Display the visualization of the Confusion Matrix.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc_auc(X, y, model, early_stopping=False):\n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(y))]\n",
    "    # predict probabilities\n",
    "    if early_stopping:\n",
    "        lr_probs = model.predict_proba(X, ntree_limit=model.best_ntree_limit)\n",
    "    else:\n",
    "        t\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y, ns_probs)\n",
    "    lr_auc = roc_auc_score(y, lr_probs)\n",
    "    # summarize scores\n",
    "    print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y, lr_probs)\n",
    "    # plot the roc curve for the model\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Chance', color='r')\n",
    "    plt.plot(lr_fpr, lr_tpr, label='ROC', color='b')\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_info(y_test, lr_probs):\n",
    "    recall = []\n",
    "    precision = []\n",
    "    accuracy = []\n",
    "    fscore = []\n",
    "    thresholds = [round(0.1*i,2) for i in range(10)]+[0.99]\n",
    "    for thres in thresholds:\n",
    "        y_pred = (lr_probs >= thres)\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        fscore.append(f1_score(y_test, y_pred))\n",
    "        \n",
    "    df_thresholds = pd.DataFrame(list(zip(thresholds,accuracy, precision, recall, fscore)),\n",
    "               columns =[\"threshold\", \"accuracy\", \"precision\", \"recall\", \"f1_score\"])\n",
    "    \n",
    "    return df_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_precision_recall(y_test, lr_probs):\n",
    "    df_thresholds = precision_recall_info(y_test, lr_probs)\n",
    "    \n",
    "    # calculate roc curves\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, lr_probs)\n",
    "    # convert to f score\n",
    "    fscore = df_thresholds[\"f1_score\"]\n",
    "    # locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    best_threshold = df_thresholds.loc[ix, 'threshold']\n",
    "    print('Best Threshold=%.2f, F-Score=%.2f' % (best_threshold, fscore[ix]))\n",
    "    # plot the roc curve for the model\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    \n",
    "    y_pred = lr_probs >= best_threshold\n",
    "    plot_cf_matrix(y_test, y_pred, best_threshold)\n",
    "\n",
    "    fig_size = (15, 5)\n",
    "    f = plt.figure()\n",
    "    axarr = f.add_subplot(1,1,1) # here is where you add the subplot to f\n",
    "\n",
    "    pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "    pyplot.plot(recall, precision, marker='.', label='Logistic')\n",
    "    pyplot.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n",
    "    # axis labels\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    pyplot.legend()\n",
    "    pyplot.close()\n",
    "    return thanhnm3_hstack_sbs([df_thresholds, f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predict prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_result_one(classifier, df_raw, cols, label=\"label\"):\n",
    "    df = df_raw.copy()\n",
    "    X_test = df.drop(columns=[label])\n",
    "    y_test = df[label]\n",
    "    X_test_trans = X_test[cols]\n",
    "    df_tmp = pd.DataFrame()\n",
    "    df_tmp['pred_prob'] = classifier.predict_proba(X_test_trans)[:,1]\n",
    "    df_tmp[label] = y_test.reset_index()[label]\n",
    "    df_concat = pd.concat([df_tmp.reset_index(drop=True), X_test.reset_index(drop=True)], axis=1)\n",
    "#     df_concat[\"y_pred\"] = (df_concat[\"pred_prob\"] >= 0.5).astype(\"int\")\n",
    "    \n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_result(classifiers, df_raw, cols, label=\"label\"):\n",
    "    df = df_raw.copy()\n",
    "    X_test = df.drop(columns=[label])\n",
    "    y_test = df[label]\n",
    "    X_test_trans = X_test[cols]\n",
    "    df_tmp = pd.DataFrame()\n",
    "    results = []\n",
    "    for classifier in classifiers:\n",
    "        results.append(classifier.predict_proba(X_test_trans)[:,1])\n",
    "    df_tmp['pred_prob'] = np.mean(results, axis=0)\n",
    "    df_tmp[label] = y_test.reset_index()[label]\n",
    "    df_concat = pd.concat([df_tmp.reset_index(drop=True), X_test.reset_index(drop=True)], axis=1)\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_error_analysis(df, x_col, bar_col=\"Total\", line_1=\"Bad Rate\", line_2=\"AUC\"):\n",
    "    df_tmp = df.copy()\n",
    "    \n",
    "    df_tmp = df_tmp.drop(index='Total')\n",
    "    \n",
    "    df_tmp[x_col] = df[x_col].astype(\"str\")\n",
    "    \n",
    "    # Create figure with secondary y-axis\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Add traces\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df_tmp[x_col], y=df_tmp[bar_col], name=\"Total\"),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_tmp[x_col], y=df_tmp[line_1], name=line_1),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "    if x_col != \"grade\":\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_tmp[x_col], y=df_tmp[line_2], name=line_2),\n",
    "            secondary_y=True,\n",
    "        )\n",
    "\n",
    "    # Set x-axis title\n",
    "    fig.update_xaxes(title_text=x_col)\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"<b>Total</b>\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"<b>Rate</b> %\", secondary_y=True,range=[0,1], tickformat=\".1%\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis_grades(df, cols=[], classifiers=None, label=\"label\", pred_prob=\"pred_prob\", bins=10):\n",
    "    df_concat = df.copy()\n",
    "    if classifiers:\n",
    "        df_concat = merge_test_result(classifiers, df, cols, label)\n",
    "    \n",
    "#     df_concat = df_concat.round({'pred_prob': 4})\n",
    "#     df_concat[\"grade\"] = pd.qcut(df_concat.pred_prob, q=bins)\n",
    "    \n",
    "    rank, range_ =pd.qcut(df_concat[pred_prob], q=bins, precision=3, retbins=True, labels=False)\n",
    "    range_ = [round(tmp, 4) for tmp in range_]\n",
    "    df_concat[\"grade\"] = [\"(\" + str(range_[i]) + \",\" + str(range_[i+1]) + \"]\" for i in rank]\n",
    "    df_tmp = (\n",
    "    df_concat\n",
    "        .groupby([\"grade\"])\n",
    "        .apply(lambda x: pd.Series({\n",
    "            'Good': (x[label]==0).sum(),\n",
    "            'Bad': (x[label]==1).sum(),\n",
    "            'Total': x[pred_prob].count(),\n",
    "            'Bad Rate': (x[label]==1).sum() / x[pred_prob].count(),\n",
    "            '% Good': (x[label]==0).sum() / df_concat[df_concat[label]==0].shape[0],\n",
    "            '% Bad': (x[label]==1).sum() / df_concat[df_concat[label]==1].shape[0],\n",
    "            '% Total': x[pred_prob].count() / df_concat.shape[0],\n",
    "            'AUC': \"\",\n",
    "            'accuracy': accuracy_score(df_concat[label].astype(\"int\"), (df_concat[pred_prob] >= x[pred_prob].max()).astype(\"int\")),\n",
    "            'recall': recall_score(df_concat[label].astype(\"int\"), (df_concat[pred_prob] >= x[pred_prob].max()-0.01).astype(\"int\")),\n",
    "            'precision': precision_score(df_concat[label].astype(\"int\"), (df_concat[pred_prob] >= (x[pred_prob].max()-0.01)).astype(\"int\")),\n",
    "    })\n",
    "    ).reset_index()\n",
    "    )\n",
    "    df_tmp.loc['Total']= df_tmp.sum(numeric_only=True, axis=0)\n",
    "    df_tmp.loc['Total', [ 'accuracy', 'recall', 'precision']] = \"\"\n",
    "    df_tmp.loc['Total', 'AUC'] = roc_auc_score(df_concat[label].astype(\"int\"), df_concat[pred_prob])\n",
    "    df_tmp.loc['Total', 'Bad Rate'] = df_tmp.loc['Total', 'Bad Rate'] / 10\n",
    "    df_tmp[['Good', 'Bad','Total']] = df_tmp[['Good', 'Bad','Total']].astype(\"int\")    \n",
    "    fig = draw_error_analysis(df_tmp, \"grade\")\n",
    "    \n",
    "    s1 = df_tmp.style\n",
    "    s1.format(precision=2)\n",
    "    s1.format(precision=0, subset=['Good', 'Bad','Total'])\n",
    "        \n",
    "    return thanhnm3_hstack_sbs([s1.to_html(), fig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(df, cols, group_col, classifiers=None, label=\"label\", num=False, threshold=0.5, bins=5):\n",
    "    df_concat = df.copy()\n",
    "    if classifiers:\n",
    "        df_concat = merge_test_result(classifiers, df, cols, label)\n",
    "    if num:\n",
    "        df_concat[group_col] = pd.qcut(df_concat[group_col], q=bins)\n",
    "    # df_concat[\"grade\"] = pd.qcut(df_concat.pred_prob, 10)\n",
    "    df_concat[\"y_pred\"] = (df_concat[\"pred_prob\"] >= threshold).astype(\"int\")\n",
    "    label = \"label\"\n",
    "    df_tmp = (\n",
    "        df_concat\n",
    "            .groupby([group_col])\n",
    "            .apply(lambda x: pd.Series({\n",
    "                'Good': (x[label]==0).sum(),\n",
    "                'Bad': (x[label]==1).sum(),\n",
    "                'Total': x[\"pred_prob\"].count(),\n",
    "                'Bad Rate': (x[label]==1).sum() / x[\"pred_prob\"].count(),\n",
    "                '% Good': (x[label]==0).sum() / df_concat[df_concat[label]==0].shape[0],\n",
    "                '% Bad': (x[label]==1).sum() / df_concat[df_concat[label]==1].shape[0],\n",
    "                '% Total': x[\"pred_prob\"].count() / df_concat.shape[0],\n",
    "                'AUC': roc_auc_score(x[\"label\"].astype(\"int\"), x[\"pred_prob\"]),\n",
    "                'accuracy': accuracy_score(x[\"label\"].astype(\"int\"), x[\"y_pred\"]),\n",
    "                'recall': recall_score(x[\"label\"].astype(\"int\"), x[\"y_pred\"]),\n",
    "                'precision': precision_score(x[\"label\"].astype(\"int\"), x[\"y_pred\"]),\n",
    "    })\n",
    "    ).reset_index()\n",
    "    ).sort_values(by=['AUC'], ascending=False)\n",
    "\n",
    "    df_tmp.loc['Total']= df_tmp.sum(numeric_only=True, axis=0)\n",
    "\n",
    "    df_tmp.loc['Total', 'AUC'] = roc_auc_score(df_concat[\"label\"].astype(\"int\"), df_concat[\"pred_prob\"])\n",
    "    df_tmp.loc['Total', 'accuracy'] = accuracy_score(df_concat[\"label\"].astype(\"int\"), df_concat[\"y_pred\"])\n",
    "    df_tmp.loc['Total', 'recall'] = recall_score(df_concat[\"label\"].astype(\"int\"), df_concat[\"y_pred\"])\n",
    "    df_tmp.loc['Total', 'precision'] = precision_score(df_concat[\"label\"].astype(\"int\"), df_concat[\"y_pred\"])\n",
    "    df_tmp.loc['Total', 'Bad Rate'] = df_tmp.loc['Total', 'Bad Rate'] / 10\n",
    "    df_tmp[['Good', 'Bad','Total']] = df_tmp[['Good', 'Bad','Total']].astype(\"int\")\n",
    "    fig = draw_error_analysis(df_tmp, group_col)\n",
    "    \n",
    "    s1 = df_tmp.style\n",
    "    s1.format(precision=2)\n",
    "    s1.format(precision=0, subset=['Good', 'Bad','Total'])\n",
    "        \n",
    "    return thanhnm3_hstack_sbs([s1.to_html(), fig])\n",
    "\n",
    "#     return draw_error_analysis(df_tmp, group_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_auc(x, label=\"label\"):\n",
    "    lr_auc = roc_auc_score(x['label'], x['pred_prob'])\n",
    "    return lr_auc\n",
    "\n",
    "def error_analysis_auc(classifiers, df, cols, ana_col, label=\"label\"):\n",
    "    df_concat = merge_test_result(classifiers, df, cols, label)\n",
    "\n",
    "    ana_col_count = df_concat.groupby(ana_col)[label].nunique()\n",
    "\n",
    "    ana_col_two_label = df_concat[~df_concat[ana_col].isin(ana_col_count[ana_col_count == 1].index.tolist())]\n",
    "    ana_col_auc_score = ana_col_two_label.groupby([ana_col]).apply(cal_auc).rename(\"AUC score\").reset_index().sort_values(by=[\"AUC score\"], ascending=False)\n",
    "\n",
    "    return ana_col_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_fine_grain(df_src, df_des, src_name=\"src\", des_name=\"des\", col=\"breakpoint\", threshold=5, full=False, is_cate=True, draw=False):\n",
    "    src = df_src.copy()\n",
    "    des = df_des.copy()\n",
    "    if is_cate:\n",
    "        src = src.fillna(value=\"Missing\")\n",
    "        des = des.fillna(value=\"Missing\")\n",
    "\n",
    "        group = src[col].nunique()\n",
    "\n",
    "        if (group > threshold) and (full==False):\n",
    "#             print(f\"Total unique {col}: {group}\")\n",
    "\n",
    "            top_ = src[col].value_counts().iloc[:threshold].index.to_list()\n",
    "\n",
    "            src.loc[~src[col].isin(top_), col] = f\"Others ({group - threshold})\"\n",
    "            des.loc[~des[col].isin(top_), col] = f\"Others ({group - threshold})\"\n",
    "\n",
    "    df2 = (\n",
    "        des\n",
    "            .groupby([col])\n",
    "            .apply(lambda x: pd.Series({\n",
    "                'des_count': x[col].count(),\n",
    "                'des_perc': x[col].count() / des.shape[0],\n",
    "        })))\n",
    "\n",
    "    df1 = (\n",
    "        src\n",
    "            .groupby([col])\n",
    "            .apply(lambda x: pd.Series({\n",
    "                'src_count': x[col].count(),\n",
    "                'src_perc': x[col].count() / src.shape[0]\n",
    "        })))\n",
    "    \n",
    "    tmp = pd.concat([df1, df2], axis=1)\n",
    "    tmp[\"src_count\"] = tmp[\"src_count\"].fillna(0)\n",
    "    tmp[\"des_count\"] = tmp[\"des_count\"].fillna(0)\n",
    "    tmp[\"des_perc\"] = tmp[\"des_perc\"].fillna(0.00000000001)\n",
    "    tmp[\"src_perc\"] = tmp[\"src_perc\"].fillna(0.00000000001)\n",
    "    \n",
    "    tmp[\"PSI\"] = (tmp[\"des_perc\"] - tmp[\"src_perc\"])*np.log((tmp[\"des_perc\"] / tmp[\"src_perc\"]))\n",
    "\n",
    "    tmp.reset_index(inplace=True)\n",
    "\n",
    "    tmp.loc['Total']= tmp.sum(numeric_only=True, axis=0)\n",
    "    \n",
    "    tmp = tmp[[col, f'src_count', f'des_count', f'src_perc', f'des_perc', 'PSI']]\n",
    "    tmp.columns = [col, f'{src_name}_count', f'{des_name}_count', f'{src_name}_perc', f'{des_name}_perc', 'PSI']\n",
    "    \n",
    "#     if is_cate == False:\n",
    "    if col == \"breakpoint\":\n",
    "        tmp[\"bucket\"] = tmp[\"breakpoint\"].str.replace(\"]\", \"\").str.split(\",\").str[-1]\n",
    "        tmp[\"bucket\"] = tmp[\"bucket\"].astype(\"float\")\n",
    "        tmp = tmp.sort_values(by=[\"bucket\"])\n",
    "        tmp = tmp.drop(columns=[\"bucket\"])\n",
    "        \n",
    "    tmp.loc['Total', col] = ''\n",
    "        \n",
    "    if draw:\n",
    "#         if is_cate:\n",
    "#             print(\"\\033[1m\", \"PSI of\", col)\n",
    "        return draw_fine_grain(tmp, src_name, des_name, col, is_cate=is_cate)\n",
    "        \n",
    "    return tmp, tmp.loc[\"Total\", \"PSI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_fine_grain_num(src, des, src_name=\"src\", des_name=\"des\", col=\"pred_prob\", bins=10, draw=False):\n",
    "    src = src[[col]].dropna().astype(\"float\").copy()\n",
    "    des = des[[col]].dropna().astype(\"float\").copy()\n",
    "    rank, range_ = pd.qcut(src[col], q=bins, precision=3, retbins=True, labels=False, duplicates=\"drop\")\n",
    "#     print(range_)\n",
    "    range_[0] = range_[0] if range_[0] < des[col].min()-0.00001 else des[col].min()-0.00001\n",
    "    range_[-1] = range_[-1] if range_[-1] > des[col].max()+0.00001 else des[col].max()+0.00001\n",
    "    \n",
    "    rank_des = pd.cut(des[col], bins=range_, labels=False).astype(\"int\")\n",
    "    \n",
    "    range_ = [round(tmp, 4) for tmp in range_]\n",
    "\n",
    "    des[\"breakpoint\"] = [\"(\" + str(range_[i]) + \",\" + str(range_[i+1]) + \"]\" for i in rank_des]\n",
    "    \n",
    "    \n",
    "    src[\"breakpoint\"] = [\"(\" + str(range_[i]) + \",\" + str(range_[i+1]) + \"]\" for i in rank]\n",
    "    \n",
    "#     if draw:\n",
    "#         print(\"\\033[1m\", \"PSI of\", col)\n",
    " \n",
    "    return cal_fine_grain(src, des, src_name, des_name, col=\"breakpoint\", is_cate=False, draw=draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_fine_grain(df, src_name=\"src\", des_name=\"des\", col=\"breakpoint\", is_cate=False):\n",
    "    total = df.loc[\"Total\", \"PSI\"]\n",
    "    fig = go.Figure()\n",
    "    draw = df.copy()\n",
    "    draw = draw.drop(index='Total')\n",
    "#     if is_cate == True:\n",
    "    draw[col] = draw[col].astype(\"string\")\n",
    "    fig.add_bar(x=draw[col],y=draw[f\"{src_name}_perc\"],opacity=0.2, width=0.7, name=\"src_perc\")\n",
    "    fig.add_bar(x=draw[col],y=draw[f\"{des_name}_perc\"], width=0.5, marker_color=\"#000080\",  name=\"des_perc\")\n",
    "\n",
    "    fig.update_layout(barmode=\"overlay\")\n",
    "    \n",
    "    # Set x-axis title\n",
    "    fig.update_xaxes(title_text=\"Bucket\")\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"Percent\")\n",
    "    width = get_plotly_bar_chart_figwidth(len(df))\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=400,\n",
    "        height=500)\n",
    "    \n",
    "    s1 = df.style\n",
    "    s1.format(precision=2)\n",
    "    s1.format(precision=0, subset=[f'{src_name}_count', f'{des_name}_count'])\n",
    "    \n",
    "    slice_ =pd.IndexSlice[draw.index, \"PSI\"]\n",
    "    \n",
    "    slice_total = pd.IndexSlice[\"Total\", \"PSI\"]\n",
    "    s1.bar(subset=slice_, color='lightgrey')\n",
    "    if (total >= 0.1) and (total <= 0.25):\n",
    "        s1.set_properties(**{'background-color': 'yellow'}, subset=slice_total)\n",
    "    elif total > 0.25:\n",
    "        s1.set_properties(**{'background-color': 'red'}, subset=slice_total)\n",
    "        \n",
    "    s1.set_table_attributes('class=\"table\"')\n",
    "    \n",
    "    return s1.to_html(), fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_grain(df, col):\n",
    "    groups = df.groupby(col)\n",
    "    group_ids = []\n",
    "    group_data = []\n",
    "    for group_id, group in groups:\n",
    "        group_ids.append(group_id)\n",
    "        group_data.append(group)\n",
    "    \n",
    "    max_psi = 0\n",
    "    max_df = pd.DataFrame()\n",
    "        \n",
    "#     if len(group_ids) <= 2:\n",
    "#         max_df, max_psi =fine_grain(src, des)\n",
    "#         psi_list.append(max_psi)\n",
    "\n",
    "    results = pd.DataFrame(columns=['group','psi'])\n",
    "\n",
    "    group_ids_revert = group_ids.copy()\n",
    "    group_data_revert = group_data.copy()\n",
    "    for i in range(len(group_ids)-2):\n",
    "        group_ids_revert.append(group_ids_revert.pop(0))\n",
    "        group_data_revert.append(group_data_revert.pop(0))\n",
    "        psi_list = []\n",
    "        for src, des in zip(group_data, group_data_revert):\n",
    "            tmp, psi = cal_fine_grain_num(src, des)\n",
    "            if max_psi < psi:\n",
    "                max_psi = psi\n",
    "                max_df = tmp\n",
    "            psi_list.append(psi)\n",
    "        \n",
    "        group_name = list(zip(group_ids, group_ids_revert))\n",
    "        result = pd.DataFrame(list(zip(group_name, psi_list)),\n",
    "                  columns=['group','psi'])\n",
    "        results = results.append(result, ignore_index=True)\n",
    "\n",
    "    if len(group_ids) == 2:        \n",
    "        max_df, max_psi = cal_fine_grain_num(group_data[0], group_data[1])\n",
    "        results = pd.DataFrame(list(zip([(group_ids[0], group_ids[1])], [max_psi])),\n",
    "                  columns=['group','psi'])\n",
    "    display(results)\n",
    "    display(draw_fine_grain(max_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_fine_grain(df, src_name=\"src\", des_name=\"des\", col=\"breakpoint\", is_cate=False):\n",
    "    total = df.loc[\"Total\", \"PSI\"]\n",
    "    fig = go.Figure()\n",
    "    draw = df.copy()\n",
    "    draw = draw.drop(index='Total')\n",
    "#     if is_cate == True:\n",
    "    draw[col] = draw[col].astype(\"string\")\n",
    "    fig.add_bar(x=draw[col],y=draw[f\"{src_name}_perc\"],opacity=0.2, width=0.7, name=\"src_perc\")\n",
    "    fig.add_bar(x=draw[col],y=draw[f\"{des_name}_perc\"], width=0.5, marker_color=\"#000080\",  name=\"des_perc\")\n",
    "\n",
    "    fig.update_layout(barmode=\"overlay\")\n",
    "    \n",
    "    # Set x-axis title\n",
    "    fig.update_xaxes(title_text=\"Bucket\")\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"Percent\")\n",
    "    width = get_plotly_bar_chart_figwidth(len(df))\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=400,\n",
    "        height=500)\n",
    "    \n",
    "    s1 = df.style\n",
    "    s1.format(precision=2)\n",
    "    s1.format(precision=0, subset=[f'{src_name}_count', f'{des_name}_count'])\n",
    "    \n",
    "    slice_ =pd.IndexSlice[draw.index, \"PSI\"]\n",
    "    \n",
    "    slice_total = pd.IndexSlice[\"Total\", \"PSI\"]\n",
    "    s1.bar(subset=slice_, color='lightgrey')\n",
    "    if (total >= 0.1) and (total <= 0.25):\n",
    "        s1.set_properties(**{'background-color': 'yellow'}, subset=slice_total)\n",
    "    elif total > 0.25:\n",
    "        s1.set_properties(**{'background-color': 'red'}, subset=slice_total)\n",
    "        \n",
    "    s1.set_table_attributes('class=\"table\"')\n",
    "    \n",
    "    return s1.to_html(), fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242.953px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
